{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3311543-7353-47fc-831e-87cabe80db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2473700-46b6-4715-a734-94e1ee2998a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2752, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2752, bias=False)\n",
       "          (down_proj): Linear(in_features=2752, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((1024,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"KoboldAI/llama2-tokenizer\")\n",
    "# Specify the path to the directory containing the model files\n",
    "model_dir = \"out/hf\"\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the target GPU device\n",
    "device = 'cuda:0'  # Change to 'cuda:1' if you want to use GPU 1\n",
    "\n",
    "# Create a custom device map that assigns all layers to the target device\n",
    "device_map = {'': device}\n",
    "model_name = \"sabareesh88/fw14k\"\n",
    "\n",
    "# Load the configuration\n",
    "config = AutoConfig.from_pretrained(model_dir)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Load the model with the quantization config\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "698281a6-3852-4674-9d5a-cb31f92685f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world’s greatest inventors had a great deal to say about the importance of their contributions to the world.\n",
      "The first inventor to make a huge contribution was Leonardo da Vinci. We already know his contribution to the world, but we don’t know exactly how much he contributed. But we can tell you that he was a genius. He was the first to use a camera obscura to take pictures. His invention was so important that after he invented it, people began to copy him and use his invention in their lives.\n",
      "The second inventor to make a remarkable contribution was the inventor and explorer, Albert Einstein. He had a lot of trouble with the laws of physics, but he also had a lot of incredible ideas. His work was so important to the world that he won the Nobel Prize in Physics in 1921.\n",
      "The third inventor we mentioned earlier was the inventor and inventor, Leonardo da Vinci. He was a genius, and his inventions were so important that he was awarded the Nobel Prize in Physics in 1903. His inventions included the “Leonardo Da Vinci Sleeping Pad,” which was a huge improvement over previous sleeping pads.\n",
      "The fourth inventor we mentioned earlier was the inventor, and the explorer, who was the first to create a steam engine. His design was so great that it was used to power ships and railroads. His invention greatly improved the lives of people, and it was so popular that he was even awarded two Nobel Prizes.\n",
      "The fifth inventor we mentioned earlier was the inventor and explorer, who was the first to use a rocket. His invention was so amazing that it helped save the lives of so many people. His invention was so powerful that it helped make space travel possible.\n",
      "The sixth inventor we mentioned earlier was the inventor of the world’s most famous painting, The Last Supper. His invention was so important that it helped save the lives of so many people. His invention helped save people’s lives and that’s why it’s important to make sure that everyone has access to it.\n",
      "The seven inventors we mentioned earlier were the inventors of the world’s largest and most powerful solar system. They were also the inventors of the world’s first nuclear reactor\n"
     ]
    }
   ],
   "source": [
    "# Example input text\n",
    "input_text = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Move inputs to GPU\n",
    "\n",
    "# Define the generation parameters\n",
    "max_new_tokens = 500  # Maximum number of new tokens to generate\n",
    "temperature = 0.7    # Sampling temperature\n",
    "top_k = 50           #\n",
    "\n",
    "# Generate text using the model's `generate` method\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        do_sample=True  # Enable sampling for more diverse outputs\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens back into text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b240e07-fced-4ebd-80d3-47dadb2e774e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c53d9e-27ae-4b47-880b-fb8cbec3968a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
